{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e3db3e",
   "metadata": {},
   "source": [
    "Step 1: Ingestion and Clause identification.\n",
    "\n",
    "    User uploads TA. Function splits user uploaded TA into meangingful clauses  \n",
    "\n",
    "Step 2: Clause Comparison. RAG 1 for Tenancy Agreement\n",
    "\n",
    "    Vector store: RAG 1 - Ideal Clauses from reference TAs\n",
    "    Compare and retrieve: Retrieve relevant RAG clauses based on similarity to clauses in user uploaded TA\n",
    "    LLM evaluation: Use LLM to compare user clause and ideal clauses. Provide feedback on TA through a report\n",
    "\n",
    "Step 3: Post-Report Q and A from User. RAG 2 for Q and A\n",
    "\n",
    "    Vector store: RAG 2 - Q and A from Database Requirements.xlsx\n",
    "    Compare and retrieve: Retrieve relevant RAG Q and A clauses based on similarity to user's question\n",
    "    LLM evaluation: Use LLM to take in additional context from RAG to improve answer.\n",
    "\n",
    "Next Step: Template recomendation? Integrating law into RAG? \n",
    "\n",
    "    Currently we only compare clauses of our RAG database with uploaded user TA. No way to compare and say what clauses must be in the TA. Maybe we can include that in the prompt, instead of using RAG for this part? Like \"Tenancy agreement should have a, b, c, d clauses at minimum and most will also have e, f clauses\" Also need to figure out how to integrate legal documents into RAG 1 to help explain legality of certain clauses. Right now it just compares clauses in user TA to clauses in RAG database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427d0aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quan-\\OneDrive\\Desktop\\Masters\\DS5105-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Core LangChain/HuggingFace RAG Imports\n",
    "from langchain_core.documents import Document \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# Import the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# SentenceTransformer is replaced by HuggingFaceEmbeddings(model_name=...)\n",
    "# faiss is replaced by FAISS from langchain_community\n",
    "# We will still need numpy for the internal dataframe processing\n",
    "# We will use the already configured SentenceTransformer model name\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize the shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce929d0-604d-4c70-a611-665ede6cd743",
   "metadata": {},
   "source": [
    "# --- RAG 1: Ideal Clauses (The 'Gold Standard' for Comparison) ---\n",
    "def build_ideal_clauses_retriever(data_directory = \"./TA_template\", \n",
    "                                  faiss_index_path = \"./faiss_index_ideal_clauses\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads, chunks, and indexes the ideal tenancy agreement PDFs (RAG 1 source).\n",
    "    Returns a LangChain FAISS Retriever.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- BUILDING RAG 1: IDEAL CLAUSES RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load Documents\n",
    "    loader = DirectoryLoader(\n",
    "        path=data_directory,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    all_documents = loader.load()\n",
    "    print(f\"Loaded {len(all_documents)} document pages.\")\n",
    "\n",
    "    # 2. Chunk Documents (using the custom, structure-aware splitter)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "    all_chunks = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(all_chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\n--- IDEAL CLAUSES CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, chunk in enumerate(all_chunks[:3]):\n",
    "        print(f\"Chunk {i+1} (Length: {len(chunk.page_content)}):\")\n",
    "        # Print the first 200 characters to keep the output manageable\n",
    "        print(f\"  {chunk.page_content[:200].replace('\\n', ' ')}...\") \n",
    "        print(f\"  Metadata: {chunk.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
    "    print(\"FAISS index for Ideal Clauses created successfully.\")\n",
    "\n",
    "    # Save the index to the repository\n",
    "    vectorstore.save_local(faiss_index_path)\n",
    "    print(f\"FAISS index saved to {faiss_index_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00aa7d0-427c-4e4f-bef8-e56311f959ca",
   "metadata": {},
   "source": [
    "# --- RAG 2: General Q&A (Excel Source) ---\n",
    "def build_general_qa_retriever(file_path, faiss_index_path=\"./faiss_index_general_qa\"):\n",
    "    \"\"\"\n",
    "    Loads data from the Excel file, converts it to Documents, and creates a FAISS retriever.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- BUILDING RAG 2: GENERAL Q&A RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load and process data (using the existing logic)\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, header=1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Using dummy data.\")\n",
    "        df = pd.DataFrame({\n",
    "            'Question': [\"How do I know if my tenancy agreement is valid?\"],\n",
    "            'Answer / Explanation': [\"Must be in writing, signed by both parties, and include essential terms.\"],\n",
    "            'Legal Commentary': [\"Only agreements containing essential terms are enforceable.\"],\n",
    "            'Government Regulation / Explanation': [\"N/A\"]\n",
    "        })\n",
    "    \n",
    "    # 2. Convert Data Rows into LangChain Documents\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        content = textwrap.dedent(f\"\"\"\n",
    "            Question: {row.get('Question', 'N/A')}\n",
    "            Answer/Explanation: {row.get('Answer / Explanation', 'N/A')}\n",
    "            Legal Context: {row.get('Legal Commentary', 'N/A')}\n",
    "            Regulation/Source: {row.get('Government Regulation / Explanation', 'N/A')}\n",
    "        \"\"\").strip()\n",
    "        \n",
    "        # Create a LangChain Document\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path, \"row_index\": index}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    print(f\"Created {len(documents)} General Q&A Documents.\")\n",
    "\n",
    "    print(\"\\n--- GENERAL Q&A CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"FAISS index for General Q&A created successfully.\")\n",
    "    \n",
    "    # Save the index to the repository\n",
    "    vectorstore.save_local(faiss_index_path)\n",
    "    print(f\"FAISS index saved to {faiss_index_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(FAISS_INDEX_PATH, K):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads the saved FAISS index and returns the retriever object for runtime use.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- LOADING RAG 1 RETRIEVER ---\")\n",
    "    \n",
    "    if not os.path.exists(FAISS_INDEX_PATH):\n",
    "        # This is a critical error if the index should be pre-built\n",
    "        raise FileNotFoundError(\n",
    "            f\"FAISS index not found at {FAISS_INDEX_PATH}. \"\n",
    "            \"Please run 'create_and_save_ideal_index()' first.\"\n",
    "        )\n",
    "\n",
    "    # 1. Load Vector Store\n",
    "    # IMPORTANT: You still need the 'embeddings' model object to load the index\n",
    "    vectorstore = FAISS.load_local(\n",
    "        FAISS_INDEX_PATH, \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization = True # Required by LangChain for loading\n",
    "    )\n",
    "    print(\"FAISS index loaded successfully.\")\n",
    "\n",
    "    # 2. Return Retriever\n",
    "    return vectorstore.as_retriever(search_kwargs = {\"k\": K})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c513d-5db1-4527-bd50-ff99bb92fda6",
   "metadata": {},
   "source": [
    "# Testing RAG 1 and 2\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_NAME = 'Database Requirements.xlsx'\n",
    "DATA_DIRECTORY = \"./TA_template\"\n",
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "GENERAL_QA_FAISS = \"./faiss_index_general_qa\" \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. BUILD THE TWO RAG SYSTEMS\n",
    "    ideal_clauses_knowledgebase = build_ideal_clauses_retriever(DATA_DIRECTORY)\n",
    "    general_qa_knowledgebase = build_general_qa_retriever(FILE_NAME)\n",
    "    ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "    general_qa_retriever = load_retriever(GENERAL_QA_FAISS, K)\n",
    "    print(\"\\n✅ BOTH RAG SYSTEMS ARE READY.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # --- SIMULATE THE PIPELINE EXECUTION ---\n",
    "    \n",
    "    # --- PHASE 1: CLAUSE IDENTIFICATION & COMPARISON (RAG 1) ---\n",
    "    print(\"\\n--- PHASE 1: COMPARING USER AGREEMENT CLAUSES ---\")\n",
    "    \n",
    "    # 1.1 Simulate a single clause from the user's uploaded document\n",
    "    # In the full app, you would run the text_splitter on the user upload here\n",
    "    user_clause_to_check = \"(b) SECURITY DEPOSIT: The Tenant shall pay the Landlord a security deposit equal to $3,000. This deposit will not accrue interest.\"\n",
    "    \n",
    "    # 1.2 Use RAG 1 to retrieve the Ideal Clause context\n",
    "    comparison_context = ideal_clauses_retriever.invoke(user_clause_to_check)\n",
    "    \n",
    "    # 1.3 LLM Call for Feedback (Simulated)\n",
    "    print(f\"\\nUser Clause: {user_clause_to_check}\")\n",
    "    print(\"\\n[SIMULATED LLM COMPARISON & FEEDBACK GENERATION]\")\n",
    "    print(f\"Retrieved Ideal Context for LLM:\")\n",
    "    for doc in comparison_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now compare the user clause to this context and generate a report.*\n",
    "\n",
    "    # --- PHASE 2: POST-FEEDBACK Q&A (RAG 2) ---\n",
    "    print(\"\\n\\n--- PHASE 2: GENERAL Q&A ---\")\n",
    "    \n",
    "    # 2.1 User enters a follow-up question\n",
    "    user_qa_query = \"What rules apply to my landlord entering my rental?\"\n",
    "    \n",
    "    # 2.2 Use RAG 2 to retrieve the General Q&A context\n",
    "    qa_context = general_qa_retriever.invoke(user_qa_query)\n",
    "    \n",
    "    # 2.3 LLM Call for Answer (Simulated)\n",
    "    print(f\"\\nUser Q&A Query: {user_qa_query}\")\n",
    "    print(\"\\n[SIMULATED LLM ANSWER GENERATION]\")\n",
    "    print(f\"Retrieved General Q&A Context for LLM:\")\n",
    "    for doc in qa_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now synthesize these into a single, cohesive answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48039408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down user uploaded TA\n",
    "def load_and_extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a file is a PDF, loads it, and extracts all text content.\n",
    "    \n",
    "    Args:\n",
    "        file_path: The local path to the user's uploaded file.\n",
    "\n",
    "    Returns:\n",
    "        A single string containing all text from the PDF.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file is not found or is not a PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading and Extracting Text from: {file_path} ---\")\n",
    "\n",
    "    # 1. Basic File Check\n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError(f\"Error: File not found at path: {file_path}\")\n",
    "\n",
    "    # 2. PDF Extension Check (Simple approach)\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(f\"Error: File is not a PDF ('.pdf' extension required).\")\n",
    "\n",
    "    try:\n",
    "        # 3. Use LangChain's PyPDFLoader for robust text extraction\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        \n",
    "        # Load all pages as a list of Document objects\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages.\")\n",
    "\n",
    "        # 4. Concatenate all page content into a single string\n",
    "        full_text = \"\\n\\n\".join(page.page_content for page in pages)\n",
    "        \n",
    "        # Simple cleanup (optional, but helps with messy PDF parsing)\n",
    "        full_text = re.sub(r'\\s{2,}', ' ', full_text) # Replace multiple spaces/newlines with single space\n",
    "        full_text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', full_text) # Preserve paragraph breaks\n",
    "\n",
    "        print(\"Text extraction complete.\")\n",
    "        return full_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch errors during PDF parsing\n",
    "        raise RuntimeError(f\"An error occurred during PDF text extraction: {e}\")\n",
    "\n",
    "def split_user_document(user_uploaded_text: str, source_name: str = \"User TA\") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Splits the raw text of the user-uploaded tenancy agreement into clause-level chunks.\n",
    "\n",
    "    Args:\n",
    "        user_uploaded_text: The raw string content of the user's document.\n",
    "        source_name: A metadata tag to identify the source (e.g., the filename).\n",
    "\n",
    "    Returns:\n",
    "        A list of LangChain Document objects, one for each clause/chunk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- SPLITTING USER DOCUMENT: {source_name} ---\")\n",
    "\n",
    "    # The same structure-aware separators used for your Ideal Clauses (RAG 1)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "\n",
    "    # 1. Convert the single raw string into a list of Documents (one initial document)\n",
    "    initial_document = [\n",
    "        Document(page_content=user_uploaded_text, metadata={\"source\": source_name})\n",
    "    ]\n",
    "\n",
    "    # 2. Split the document based on the clause structure\n",
    "    user_chunks = text_splitter.split_documents(initial_document)\n",
    "    \n",
    "    print(f\"User TA split into {len(user_chunks)} clause-level chunks.\")\n",
    "    \n",
    "    # Optional: Print a preview of the first few chunks\n",
    "    for i, chunk in enumerate(user_chunks[:3]):\n",
    "        print(f\"  Chunk {i+1} (Length: {len(chunk.page_content)}): {chunk.page_content[:150].replace('\\n', ' ')}...\")\n",
    "    \n",
    "    return user_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84269562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully from environment variable.\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "# --- LLM CONFIGURATION for OpenAI ---\n",
    "# Recommended model for this task (fast and good at structured output)\n",
    "LLM_MODEL = \"gpt-4o-mini\" \n",
    "MAX_RETRIES = 3\n",
    "load_dotenv()  # Load environment variables from a .env file if present\n",
    "# Get the API key from the environment variable\n",
    "api_key_from_env = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key_from_env:\n",
    "    try:\n",
    "        # Pass the key explicitly to ensure the client is initialized correctly\n",
    "        client = OpenAI(api_key=api_key_from_env)\n",
    "        print(\"OpenAI client initialized successfully from environment variable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Failed to initialize OpenAI client even with key. Error: {e}\")\n",
    "        client = None\n",
    "else:\n",
    "    # If the key is not found, print a clear warning and set client to None\n",
    "    print(\"OPENAI_API_KEY environment variable is NOT set.\")\n",
    "    client = None\n",
    "\n",
    "def llm_compare_and_critique_openai(\n",
    "    user_clause_text: str, \n",
    "    ideal_context_docs: List[Document]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compares a user's tenancy clause against retrieved ideal context using the OpenAI API.\n",
    "    \n",
    "    The function uses structured JSON output enforcement for reliability.\n",
    "\n",
    "    Args:\n",
    "        user_clause_text: The content of the user's clause chunk.\n",
    "        ideal_context_docs: A list of relevant ideal clauses retrieved from the RAG 1 Vector Store.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the feedback, risk level, and suggestions.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return {\n",
    "            \"clause_summary\": \"API Initialization Failed\",\n",
    "            \"risk_level\": \"HIGH\",\n",
    "            \"feedback\": \"OpenAI client is not configured correctly. Check API key.\",\n",
    "            \"suggestion\": \"Set the OPENAI_API_KEY environment variable.\"\n",
    "        }\n",
    "\n",
    "    print(f\"\\n[CRITIQUE] Analyzing clause with GPT: {user_clause_text[:50]}...\")\n",
    "\n",
    "    # 1. FORMAT THE CONTEXT FOR THE LLM\n",
    "    context_str = \"\\n---\\n\".join([doc.page_content for doc in ideal_context_docs])\n",
    "    \n",
    "    # 2. DEFINE SYSTEM INSTRUCTION\n",
    "    # This sets the persona and output constraints.\n",
    "    system_instruction = (\n",
    "        \"You are a world-class legal analyst specializing in residential tenancy agreements. \"\n",
    "        \"Your task is to compare a provided 'User Clause' against 'Ideal Clause Examples' \"\n",
    "        \"and generate structured, actionable feedback. Be concise, professional, and focus only on deviations or missing protections for the tenant. \"\n",
    "        \"Your entire output MUST be a single, valid JSON object that strictly adheres to the provided JSON Schema.\"\n",
    "    )\n",
    "\n",
    "    # 3. DEFINE THE USER QUERY (THE CORE PROMPT)\n",
    "    user_query = textwrap.dedent(f\"\"\"\n",
    "        Please analyze the following 'User Clause' and compare it to the 'Ideal Clause Examples'.\n",
    "\n",
    "        **USER CLAUSE TO CRITIQUE:**\n",
    "        ---\n",
    "        {user_clause_text}\n",
    "        ---\n",
    "\n",
    "        **IDEAL CLAUSE EXAMPLES (RAG Context):**\n",
    "        ---\n",
    "        {context_str}\n",
    "        ---\n",
    "\n",
    "        Based on your comparison, provide the analysis in the specified JSON format.\n",
    "    \"\"\")\n",
    "\n",
    "    # 4. DEFINE THE JSON SCHEMA\n",
    "    response_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"clause_summary\": {\"type\": \"string\", \"description\": \"A brief (1-sentence) summary of the user clause's main topic.\"},\n",
    "            \"risk_level\": {\"type\": \"string\", \"enum\": [\"LOW\", \"MEDIUM\", \"HIGH\"], \"description\": \"The risk level (LOW, MEDIUM, or HIGH) for the tenant based on deviations from the ideal.\"},\n",
    "            \"feedback\": {\"type\": \"string\", \"description\": \"Specific, actionable criticism on what is missing or concerning in the User Clause.\"},\n",
    "            \"suggestion\": {\"type\": \"string\", \"description\": \"A brief sentence on how the user should attempt to modify the clause.\"}\n",
    "        },\n",
    "        \"required\": [\"clause_summary\", \"risk_level\", \"feedback\", \"suggestion\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    # 5. EXECUTE API CALL WITH EXPONENTIAL BACKOFF\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": user_query}\n",
    "                ],\n",
    "                # Use the response_format tool for guaranteed JSON output (GPT-4o/GPT-4 models)\n",
    "                response_format={\"type\": \"json_object\"}, \n",
    "                # Note: The JSON structure is also implicitly constrained by the prompt and system instruction.\n",
    "                temperature=0.0 # Use low temperature for analytical tasks\n",
    "            )\n",
    "            \n",
    "            # Extract and parse the JSON response text\n",
    "            # The entire output text should be a single JSON string\n",
    "            json_text = response.choices[0].message.content\n",
    "            parsed_json = json.loads(json_text)\n",
    "            print(f\"... Successful critique generated on attempt {attempt + 1}.\")\n",
    "            return parsed_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"OpenAI API Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"OpenAI API failed after {MAX_RETRIES} attempts.\")\n",
    "                return {\n",
    "                    \"clause_summary\": \"Analysis Failed\",\n",
    "                    \"risk_level\": \"HIGH\",\n",
    "                    \"feedback\": f\"Unable to generate critique after {MAX_RETRIES} attempts. Error: {e}\",\n",
    "                    \"suggestion\": \"Check your API key, model permissions, and rate limits.\"\n",
    "                }\n",
    "    \n",
    "    # Should be unreachable\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e0c79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_llm_report(data: List[Dict[str, Any]]):\n",
    "    report_lines = [\"# 📄 Comprehensive Tenancy Agreement Analysis Report\\n\"]\n",
    "    issue_counter = 1\n",
    "    \n",
    "    for clause_feedback in data:\n",
    "        # Check if the structure we expect (top-level 'feedback' key) is present\n",
    "        feedback_data = clause_feedback.get('feedback', {})\n",
    "        \n",
    "        # We need a way to summarize *which clause* this is, but the raw output doesn't \n",
    "        # explicitly provide the clause text. We'll use a placeholder structure for now.\n",
    "        report_lines.append(f\"## 🛠️ Analysis of Clause {issue_counter}\\n\")\n",
    "        \n",
    "        has_content = False\n",
    "        \n",
    "        # 1. Process Deviations\n",
    "        deviations = feedback_data.get('deviations', [])\n",
    "        if deviations:\n",
    "            report_lines.append(\"### 🔴 Deviations/High Risk Issues:\\n\")\n",
    "            for dev in deviations:\n",
    "                issue = dev.get('issue', 'N/A')\n",
    "                description = dev.get('description', 'No description provided.')\n",
    "                report_lines.append(f\"* **{issue}:** {description}\\n\")\n",
    "            has_content = True\n",
    "\n",
    "        # 2. Process Missing Protections (or similar structures like vague_terms)\n",
    "        missing_protections = feedback_data.get('missing_protections', [])\n",
    "        if not missing_protections:\n",
    "            # Check for other potential keys used by the LLM, like 'missingProtections' (camelCase)\n",
    "            missing_protections = feedback_data.get('missingProtections', [])\n",
    "        \n",
    "        vague_terms = feedback_data.get('vague_terms', [])\n",
    "        \n",
    "        if missing_protections or vague_terms:\n",
    "            report_lines.append(\"\\n### 🟡 Missing Protections/Vague Terms:\\n\")\n",
    "            \n",
    "            # Combine all non-deviation issues for a clear list\n",
    "            all_other_issues = missing_protections + vague_terms\n",
    "            \n",
    "            for issue_data in all_other_issues:\n",
    "                # The keys change slightly here ('protection' or 'issue')\n",
    "                issue = issue_data.get('protection') or issue_data.get('issue', 'N/A')\n",
    "                description = issue_data.get('description', 'No description provided.')\n",
    "                report_lines.append(f\"* **{issue}:** {description}\\n\")\n",
    "            has_content = True\n",
    "        \n",
    "        # 3. Add separator and increment counter\n",
    "        if has_content:\n",
    "            report_lines.append(\"\\n---\\n\")\n",
    "            issue_counter += 1\n",
    "        \n",
    "    # If the LLM output was perfectly fine, the list might be empty.\n",
    "    if issue_counter == 1:\n",
    "        return \"# ✅ Analysis Complete: No significant deviations found in the provided clauses.\"\n",
    "        \n",
    "    return \"\".join(report_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee16b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "K = 3\n",
    "ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "\n",
    "# Function Integrating LLM and RAG 1\n",
    "def generate_ta_report(USER_UPLOADED_FILE_PATH, ideal_clauses_retriever):\n",
    "    try:\n",
    "        # 1. LOAD & EXTRACT TEXT from the PDF\n",
    "        full_user_document_text = load_and_extract_pdf_text(USER_UPLOADED_FILE_PATH)\n",
    "\n",
    "        # 2. SPLIT the extracted text into clauses\n",
    "        user_clause_chunks = split_user_document(\n",
    "            full_user_document_text, \n",
    "            source_name=USER_UPLOADED_FILE_PATH\n",
    "        )\n",
    "\n",
    "        # 3. Loop through ALL user clauses for comparison (The RAG Core)\n",
    "        feedback_report = []\n",
    "\n",
    "        for user_clause in user_clause_chunks:\n",
    "            # Use RAG 1 to retrieve the Ideal Clause context\n",
    "            comparison_context = ideal_clauses_retriever.invoke(user_clause.page_content)\n",
    "            \n",
    "            # 4. LLM Call for Feedback (Simulated)\n",
    "            feedback = llm_compare_and_critique_openai(user_clause.page_content, comparison_context)\n",
    "            feedback_report.append(feedback)\n",
    "\n",
    "        print(\"\\n--- Phase 1 Complete: Analysis ready. ---\")\n",
    "        # final_report = format_llm_report(feedback_report)\n",
    "        return feedback_report # Changed this from final_report\n",
    "        \n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        # Handle the specific errors raised by the extraction function\n",
    "        print(f\"\\nFATAL ERROR DURING FILE PROCESSING: {e}\")\n",
    "        # You would typically stop processing here and inform the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3402b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "GENERAL_QA_FAISS = \"./faiss_index_general_qa\" \n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "general_qa_retriever = load_retriever(GENERAL_QA_FAISS, K)\n",
    "\n",
    "def answer_contextual_question_openai(\n",
    "    user_question: str, \n",
    "    general_qa_retriever: object,\n",
    "    ta_report: Optional[List[Dict]] = None, # Optional TA Report (Phase 1 output)\n",
    "    past_messages: Optional[List[Dict[str, str]]] = None # Optional Chat History\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Answers a user question using RAG 2, the TA critique report, and chat history.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"OpenAI client is not configured correctly. Check API key.\"\n",
    "\n",
    "    print(f\"\\n[Q&A] Answering contextual question with GPT: {user_question[:50]}...\")\n",
    "\n",
    "    # 1. Use RAG 2 to retrieve the General Q&A context\n",
    "    qa_context = general_qa_retriever.invoke(user_question)\n",
    "    \n",
    "    # 2. Format Contexts\n",
    "    general_context_str = \"\\n---\\n\".join([doc.page_content for doc in qa_context])\n",
    "    report_context_str = (ta_report or []) # Format report if provided\n",
    "\n",
    "    # 3. DEFINE THE LLM MESSAGE LIST (Chat History Integration)\n",
    "    \n",
    "    # Update System Instruction to acknowledge all contexts\n",
    "    system_instruction = (\n",
    "        \"You are an expert legal assistant specializing in residential tenancy agreements. \"\n",
    "        \"Your response must be based on the provided context, which includes the **General Law Context** (RAG data) \"\n",
    "        \"and, if present, the **User Document Analysis Report** (specific critiques). \"\n",
    "        \"Maintain the flow of the conversation history. If the user's question relates to a flagged clause, \"\n",
    "        \"prioritize the information from the Analysis Report. Be concise and professional.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction}\n",
    "    ]\n",
    "\n",
    "    # Add past conversation messages if they exist\n",
    "    if past_messages:\n",
    "        messages.extend(past_messages) # Extends the list with previous turns\n",
    "\n",
    "    # 4. Construct the Final User Query (containing all RAG context)\n",
    "    final_user_prompt = textwrap.dedent(f\"\"\"\n",
    "        Please answer the FINAL USER QUESTION using the context provided below.\n",
    "\n",
    "        **THIS IS A REPORT ON THE USER'S TENANCY AGREEMENT (Specific Critiques):**\n",
    "        ---\n",
    "        {report_context_str}\n",
    "        \n",
    "        **GENERAL Q&A CONTEXT (RAG Retrieved):**\n",
    "        ---\n",
    "        {general_context_str}\n",
    "        ---\n",
    "\n",
    "        **FINAL USER QUESTION:** {user_question}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Add the final, context-rich user prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": final_user_prompt})\n",
    "\n",
    "    # 5. EXECUTE API CALL WITH EXPONENTIAL BACKOFF\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "         \n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=messages, # Now uses the full history list\n",
    "                temperature=0.0\n",
    "            )\n",
    "            answer_text = response.choices[0].message.content\n",
    "            \n",
    "            print(f\"... Successful answer generated on attempt {attempt + 1}.\")\n",
    "            return answer_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"OpenAI API Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"OpenAI API failed after {MAX_RETRIES} attempts.\")\n",
    "                return f\"Unable to generate answer after {MAX_RETRIES} attempts. Error: {e}\"\n",
    "    \n",
    "    return \"Unknown error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d087056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n",
      "\n",
      "✅ RAG 1 IS READY.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Loading and Extracting Text from: ./TA_sample/TA3.pdf ---\n",
      "Successfully loaded 2 pages.\n",
      "Text extraction complete.\n",
      "\n",
      "--- SPLITTING USER DOCUMENT: ./TA_sample/TA3.pdf ---\n",
      "User TA split into 8 clause-level chunks.\n",
      "  Chunk 1 (Length: 633): Disclaimer: This is a general document which may not be appropriate for use in all cases. When in doubt, please seek legal advice. In the event of a d...\n",
      "  Chunk 2 (Length: 514): Singapore This Agreement is made on the day of between (hereinafter known as “The Landlord”) on the one part and (hereinafter known as “The Tenant”) o...\n",
      "  Chunk 3 (Length: 715): 2. Such deposit shall be refundable at the end of the term after deductions for damages caused by the Tenant, if any. 3. The Tenant must pay the month...\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: Disclaimer: This is a general document which may n...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: Singapore This Agreement is made on the day of bet...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 2. Such deposit shall be refundable at the end of ...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 6. The above premises are strictly not for immoral...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 7.2 In respect of any change in the particulars, i...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 9. The Tenant is not allowed to sublet the premise...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 12. The Landlord shall pay their representing sale...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 16. The Tenant must produce original/photocopy of ...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "--- Phase 1 Complete: Analysis ready. ---\n"
     ]
    }
   ],
   "source": [
    "# Testing RAG 1 with LLM for full TA Report Generation\n",
    "\n",
    "# --- Configuration ---\n",
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "# Initialize RAG 1\n",
    "ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "\n",
    "print(\"\\n✅ RAG 1 IS READY.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "    \n",
    "# --- Placeholder: Replace this with the actual path to the user's uploaded file ---\n",
    "USER_UPLOADED_FILE_PATH = \"./TA_sample/TA3.pdf\" \n",
    "    # Note: In a real-world application, this path comes from your web/API framework after the user submits the file.\n",
    "\n",
    "# Generate the TA Report\n",
    "ta_report = generate_ta_report(USER_UPLOADED_FILE_PATH, ideal_clauses_retriever)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

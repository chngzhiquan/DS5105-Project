{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427d0aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chng Zhi Quan\\OneDrive\\Desktop\\Masters\\Courses\\Sem 1\\DS5105-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import re\n",
    "# Core LangChain/HuggingFace RAG Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# SentenceTransformer is replaced by HuggingFaceEmbeddings(model_name=...)\n",
    "# faiss is replaced by FAISS from langchain_community\n",
    "# We will still need numpy for the internal dataframe processing\n",
    "# We will use the already configured SentenceTransformer model name\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize the shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0ee2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG 1: Ideal Clauses (The 'Gold Standard' for Comparison) ---\n",
    "def build_ideal_clauses_retriever(data_directory=\"./TA_data\"):\n",
    "    \"\"\"\n",
    "    Loads, chunks, and indexes the ideal tenancy agreement PDFs (RAG 1 source).\n",
    "    Returns a LangChain FAISS Retriever.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- BUILDING RAG 1: IDEAL CLAUSES RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load Documents\n",
    "    loader = DirectoryLoader(\n",
    "        path=data_directory,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    all_documents = loader.load()\n",
    "    print(f\"Loaded {len(all_documents)} document pages.\")\n",
    "\n",
    "    # 2. Chunk Documents (using the custom, structure-aware splitter)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "    all_chunks = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(all_chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\n--- IDEAL CLAUSES CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, chunk in enumerate(all_chunks[:3]):\n",
    "        print(f\"Chunk {i+1} (Length: {len(chunk.page_content)}):\")\n",
    "        # Print the first 200 characters to keep the output manageable\n",
    "        print(f\"  {chunk.page_content[:200].replace('\\n', ' ')}...\") \n",
    "        print(f\"  Metadata: {chunk.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
    "    print(\"FAISS index for Ideal Clauses created successfully.\")\n",
    "    \n",
    "    # Return the retriever instance\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a1251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document # Import required for conversion\n",
    "\n",
    "# --- RAG 2: General Q&A (Excel Source) ---\n",
    "def build_general_qa_retriever(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from the Excel file, converts it to Documents, and creates a FAISS retriever.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- BUILDING RAG 2: GENERAL Q&A RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load and process data (using the existing logic)\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, header=1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Using dummy data.\")\n",
    "        df = pd.DataFrame({\n",
    "            'Question': [\"How do I know if my tenancy agreement is valid?\"],\n",
    "            'Answer / Explanation': [\"Must be in writing, signed by both parties, and include essential terms.\"],\n",
    "            'Legal Commentary': [\"Only agreements containing essential terms are enforceable.\"],\n",
    "            'Government Regulation / Explanation': [\"N/A\"]\n",
    "        })\n",
    "    \n",
    "    # 2. Convert Data Rows into LangChain Documents\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        content = textwrap.dedent(f\"\"\"\n",
    "            Question: {row.get('Question', 'N/A')}\n",
    "            Answer/Explanation: {row.get('Answer / Explanation', 'N/A')}\n",
    "            Legal Context: {row.get('Legal Commentary', 'N/A')}\n",
    "            Regulation/Source: {row.get('Government Regulation / Explanation', 'N/A')}\n",
    "        \"\"\").strip()\n",
    "        \n",
    "        # Create a LangChain Document\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path, \"row_index\": index}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    print(f\"Created {len(documents)} General Q&A Documents.\")\n",
    "\n",
    "    print(\"\\n--- GENERAL Q&A CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"FAISS index for General Q&A created successfully.\")\n",
    "    \n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e25849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BUILDING RAG 1: IDEAL CLAUSES RETRIEVER ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      " 57%|█████▋    | 4/7 [00:00<00:00, 22.32it/s]Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 5 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 document pages.\n",
      "Split into 108 chunks.\n",
      "\n",
      "--- IDEAL CLAUSES CHUNK PREVIEW (First 3) ---\n",
      "Chunk 1 (Length: 903):\n",
      "  RENTAL AGREEMENT PROBLEMATIC RENTAL TERMS LANDLORD: ABC Property Company TENANT: John Doe RENTAL AMOUNT: S$3,500 per month SECURITY DEPOSIT: S$17,500 (5 months rent) - EXCESSIVE! LEASE PERIOD: 12 mont...\n",
      "  Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-31T22:29:15+08:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-31T22:29:15+08:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'TA_data\\\\TA1.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n",
      "--------------------\n",
      "Chunk 2 (Length: 879):\n",
      "  RESIDENTIAL TENANCY AGREEMENT (Singapore) 1. PARTIES TO THE AGREEMENT LANDLORD: Mr. Tan Ah Beng NRIC: S1234567A Address: 123 Orchard Road, #15-01, Singapore 238123 Contact: +65 9123 4567 TENANT: Ms. S...\n",
      "  Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-31T22:29:15+08:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-31T22:29:15+08:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'TA_data\\\\TA2.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n",
      "--------------------\n",
      "Chunk 3 (Length: 665):\n",
      "  4. TERMS AND CONDITIONS 4.1 The Tenant shall pay the monthly rent punctually without any deduction or set-off. 4.2 The security deposit shall be refunded within 14 days after the expiry of this agreem...\n",
      "  Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-08-31T22:29:15+08:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-08-31T22:29:15+08:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'TA_data\\\\TA2.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n",
      "--------------------\n",
      "FAISS index for Ideal Clauses created successfully.\n",
      "\n",
      "--- BUILDING RAG 2: GENERAL Q&A RETRIEVER ---\n",
      "Created 53 General Q&A Documents.\n",
      "\n",
      "--- GENERAL Q&A CHUNK PREVIEW (First 3) ---\n",
      "Chunk 1:\n",
      "Question: How do I know if my tenancy agreement is valid?\n",
      "Answer/Explanation: Must be in writing, signed by both parties, and include essential terms (rent, duration, responsibilities).\n",
      "Legal Context: Only agreements containing essential terms are enforceable.\n",
      "Regulation/Source: nan\n",
      "  Metadata: {'source': 'Database Requirements.xlsx', 'row_index': 0}\n",
      "--------------------\n",
      "Chunk 2:\n",
      "Question: What should I do before signing a tenancy agreement?\n",
      "Answer/Explanation: Review all terms carefully; ensure clarity; consider legal advice. If there is an inventory list, thoroughly check the condition of the items and retain photo or video evidence.\n",
      "Legal Context: Legal review reduces risk of unenforceable clauses.\n",
      "Regulation/Source: nan\n",
      "  Metadata: {'source': 'Database Requirements.xlsx', 'row_index': 1}\n",
      "--------------------\n",
      "Chunk 3:\n",
      "Question: Can my landlord enter my property without notice?\n",
      "Answer/Explanation: Landlord must give reasonable notice (usually 24h), except emergencies. It is recommended to document the notice period in the agreement.\n",
      "Legal Context: Court upheld tenant’s right to quiet enjoyment.\n",
      "Regulation/Source: HDB/URA – Landlord must respect tenant’s quiet enjoyment; entry should be scheduled and reasonable\n",
      "  Metadata: {'source': 'Database Requirements.xlsx', 'row_index': 2}\n",
      "--------------------\n",
      "FAISS index for General Q&A created successfully.\n",
      "\n",
      "✅ BOTH RAG SYSTEMS ARE READY.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- PHASE 1: COMPARING USER AGREEMENT CLAUSES ---\n",
      "\n",
      "User Clause: (b) SECURITY DEPOSIT: The Tenant shall pay the Landlord a security deposit equal to $3,000. This deposit will not accrue interest.\n",
      "\n",
      "[SIMULATED LLM COMPARISON & FEEDBACK GENERATION]\n",
      "Retrieved Ideal Context for LLM:\n",
      "  - Length: 982. Source: TA_data\\TA7.pdf\n",
      "  - Length: 894. Source: TA_data\\TA6.pdf\n",
      "  - Length: 811. Source: TA_data\\TA3.pdf\n",
      "\n",
      "\n",
      "--- PHASE 2: GENERAL Q&A ---\n",
      "\n",
      "User Q&A Query: What rules apply to my landlord entering my rental?\n",
      "\n",
      "[SIMULATED LLM ANSWER GENERATION]\n",
      "Retrieved General Q&A Context for LLM:\n",
      "  - Length: 506. Source: Database Requirements.xlsx\n",
      "  - Length: 398. Source: Database Requirements.xlsx\n",
      "  - Length: 411. Source: Database Requirements.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Testing RAG 1 and 2\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_NAME = 'Database Requirements.xlsx'\n",
    "DATA_DIRECTORY = \"./TA_data\" \n",
    "\n",
    "# Initialize shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. BUILD THE TWO RAG SYSTEMS\n",
    "    ideal_clauses_retriever = build_ideal_clauses_retriever(DATA_DIRECTORY)\n",
    "    general_qa_retriever = build_general_qa_retriever(FILE_NAME)\n",
    "\n",
    "    print(\"\\n✅ BOTH RAG SYSTEMS ARE READY.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # --- SIMULATE THE PIPELINE EXECUTION ---\n",
    "    \n",
    "    # --- PHASE 1: CLAUSE IDENTIFICATION & COMPARISON (RAG 1) ---\n",
    "    print(\"\\n--- PHASE 1: COMPARING USER AGREEMENT CLAUSES ---\")\n",
    "    \n",
    "    # 1.1 Simulate a single clause from the user's uploaded document\n",
    "    # In the full app, you would run the text_splitter on the user upload here\n",
    "    user_clause_to_check = \"(b) SECURITY DEPOSIT: The Tenant shall pay the Landlord a security deposit equal to $3,000. This deposit will not accrue interest.\"\n",
    "    \n",
    "    # 1.2 Use RAG 1 to retrieve the Ideal Clause context\n",
    "    comparison_context = ideal_clauses_retriever.invoke(user_clause_to_check)\n",
    "    \n",
    "    # 1.3 LLM Call for Feedback (Simulated)\n",
    "    print(f\"\\nUser Clause: {user_clause_to_check}\")\n",
    "    print(\"\\n[SIMULATED LLM COMPARISON & FEEDBACK GENERATION]\")\n",
    "    print(f\"Retrieved Ideal Context for LLM:\")\n",
    "    for doc in comparison_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now compare the user clause to this context and generate a report.*\n",
    "\n",
    "    # --- PHASE 2: POST-FEEDBACK Q&A (RAG 2) ---\n",
    "    print(\"\\n\\n--- PHASE 2: GENERAL Q&A ---\")\n",
    "    \n",
    "    # 2.1 User enters a follow-up question\n",
    "    user_qa_query = \"What rules apply to my landlord entering my rental?\"\n",
    "    \n",
    "    # 2.2 Use RAG 2 to retrieve the General Q&A context\n",
    "    qa_context = general_qa_retriever.invoke(user_qa_query)\n",
    "    \n",
    "    # 2.3 LLM Call for Answer (Simulated)\n",
    "    print(f\"\\nUser Q&A Query: {user_qa_query}\")\n",
    "    print(\"\\n[SIMULATED LLM ANSWER GENERATION]\")\n",
    "    print(f\"Retrieved General Q&A Context for LLM:\")\n",
    "    for doc in qa_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now synthesize these into a single, cohesive answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ingestion and Clause identification. \n",
    "## User TA clauses: Split user uploaded TA into meangingful clauses  \n",
    "\n",
    "# Step 2: Clause Comparision. RAG for Tenancy Agreement\n",
    "## Vector store: RAG 1 - Ideal Clauses from reference TAs\n",
    "## Compare and retrieve: Content of user clause compared to most relevant ideal clauses\n",
    "## LLM evaluation: Use LLM to compare user clause and ideal clauses \n",
    "\n",
    "# Step 3: Post-Feedback Q and A. RAG for Q and A\n",
    "## Vector store: RAG 2 - Q and A from Database Requirements.xlsx\n",
    "## User interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48039408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document # Needed to wrap the user text\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os # To handle file paths\n",
    "\n",
    "def load_and_extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a file is a PDF, loads it, and extracts all text content.\n",
    "    \n",
    "    Args:\n",
    "        file_path: The local path to the user's uploaded file.\n",
    "\n",
    "    Returns:\n",
    "        A single string containing all text from the PDF.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file is not found or is not a PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading and Extracting Text from: {file_path} ---\")\n",
    "\n",
    "    # 1. Basic File Check\n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError(f\"Error: File not found at path: {file_path}\")\n",
    "\n",
    "    # 2. PDF Extension Check (Simple approach)\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(f\"Error: File is not a PDF ('.pdf' extension required).\")\n",
    "\n",
    "    try:\n",
    "        # 3. Use LangChain's PyPDFLoader for robust text extraction\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        \n",
    "        # Load all pages as a list of Document objects\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages.\")\n",
    "\n",
    "        # 4. Concatenate all page content into a single string\n",
    "        full_text = \"\\n\\n\".join(page.page_content for page in pages)\n",
    "        \n",
    "        # Simple cleanup (optional, but helps with messy PDF parsing)\n",
    "        full_text = re.sub(r'\\s{2,}', ' ', full_text) # Replace multiple spaces/newlines with single space\n",
    "        full_text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', full_text) # Preserve paragraph breaks\n",
    "\n",
    "        print(\"Text extraction complete.\")\n",
    "        return full_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch errors during PDF parsing\n",
    "        raise RuntimeError(f\"An error occurred during PDF text extraction: {e}\")\n",
    "\n",
    "def split_user_document(user_uploaded_text: str, source_name: str = \"User TA\") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Splits the raw text of the user-uploaded tenancy agreement into clause-level chunks.\n",
    "\n",
    "    Args:\n",
    "        user_uploaded_text: The raw string content of the user's document.\n",
    "        source_name: A metadata tag to identify the source (e.g., the filename).\n",
    "\n",
    "    Returns:\n",
    "        A list of LangChain Document objects, one for each clause/chunk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- SPLITTING USER DOCUMENT: {source_name} ---\")\n",
    "\n",
    "    # The same structure-aware separators used for your Ideal Clauses (RAG 1)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "\n",
    "    # 1. Convert the single raw string into a list of Documents (one initial document)\n",
    "    initial_document = [\n",
    "        Document(page_content=user_uploaded_text, metadata={\"source\": source_name})\n",
    "    ]\n",
    "\n",
    "    # 2. Split the document based on the clause structure\n",
    "    user_chunks = text_splitter.split_documents(initial_document)\n",
    "    \n",
    "    print(f\"User TA split into {len(user_chunks)} clause-level chunks.\")\n",
    "    \n",
    "    # Optional: Print a preview of the first few chunks\n",
    "    for i, chunk in enumerate(user_chunks[:3]):\n",
    "        print(f\"  Chunk {i+1} (Length: {len(chunk.page_content)}): {chunk.page_content[:150].replace('\\n', ' ')}...\")\n",
    "    \n",
    "    return user_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

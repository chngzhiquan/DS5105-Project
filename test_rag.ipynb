{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e3db3e",
   "metadata": {},
   "source": [
    "Step 1: Ingestion and Clause identification.\n",
    "\n",
    "    User uploads TA. Function splits user uploaded TA into meangingful clauses  \n",
    "\n",
    "Step 2: Clause Comparison. RAG 1 for Tenancy Agreement\n",
    "\n",
    "    Vector store: RAG 1 - Ideal Clauses from reference TAs\n",
    "    Compare and retrieve: Retrieve relevant RAG clauses based on similarity to clauses in user uploaded TA\n",
    "    LLM evaluation: Use LLM to compare user clause and ideal clauses. Provide feedback on TA through a report\n",
    "\n",
    "Step 3: Post-Report Q and A from User. RAG 2 for Q and A\n",
    "\n",
    "    Vector store: RAG 2 - Q and A from Database Requirements.xlsx\n",
    "    Compare and retrieve: Retrieve relevant RAG Q and A clauses based on similarity to user's question\n",
    "    LLM evaluation: Use LLM to take in additional context from RAG to improve answer.\n",
    "\n",
    "Next Step: Template recomendation? Integrating law into RAG? \n",
    "\n",
    "    Currently we only compare clauses of our RAG database with uploaded user TA. No way to compare and say what clauses must be in the TA. Maybe we can include that in the prompt, instead of using RAG for this part? Like \"Tenancy agreement should have a, b, c, d clauses at minimum and most will also have e, f clauses\" Also need to figure out how to integrate legal documents into RAG 1 to help explain legality of certain clauses. Right now it just compares clauses in user TA to clauses in RAG database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d0aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quan-\\OneDrive\\Desktop\\Masters\\DS5105-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Core LangChain/HuggingFace RAG Imports\n",
    "from langchain_core.documents import Document \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# Import the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# SentenceTransformer is replaced by HuggingFaceEmbeddings(model_name=...)\n",
    "# faiss is replaced by FAISS from langchain_community\n",
    "# We will still need numpy for the internal dataframe processing\n",
    "# We will use the already configured SentenceTransformer model name\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize the shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce929d0-604d-4c70-a611-665ede6cd743",
   "metadata": {},
   "source": [
    "# --- RAG 1: Ideal Clauses (The 'Gold Standard' for Comparison) ---\n",
    "def build_ideal_clauses_retriever(data_directory = \"./TA_template\", \n",
    "                                  faiss_index_path = \"./faiss_index_ideal_clauses\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads, chunks, and indexes the ideal tenancy agreement PDFs (RAG 1 source).\n",
    "    Returns a LangChain FAISS Retriever.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- BUILDING RAG 1: IDEAL CLAUSES RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load Documents\n",
    "    loader = DirectoryLoader(\n",
    "        path=data_directory,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    all_documents = loader.load()\n",
    "    print(f\"Loaded {len(all_documents)} document pages.\")\n",
    "\n",
    "    # 2. Chunk Documents (using the custom, structure-aware splitter)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "    all_chunks = text_splitter.split_documents(all_documents)\n",
    "    print(f\"Split into {len(all_chunks)} chunks.\")\n",
    "\n",
    "    print(\"\\n--- IDEAL CLAUSES CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, chunk in enumerate(all_chunks[:3]):\n",
    "        print(f\"Chunk {i+1} (Length: {len(chunk.page_content)}):\")\n",
    "        # Print the first 200 characters to keep the output manageable\n",
    "        print(f\"  {chunk.page_content[:200].replace('\\n', ' ')}...\") \n",
    "        print(f\"  Metadata: {chunk.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n",
    "    print(\"FAISS index for Ideal Clauses created successfully.\")\n",
    "\n",
    "    # Save the index to the repository\n",
    "    vectorstore.save_local(faiss_index_path)\n",
    "    print(f\"FAISS index saved to {faiss_index_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00aa7d0-427c-4e4f-bef8-e56311f959ca",
   "metadata": {},
   "source": [
    "# --- RAG 2: General Q&A (Excel Source) ---\n",
    "def build_general_qa_retriever(file_path, faiss_index_path=\"./faiss_index_general_qa\"):\n",
    "    \"\"\"\n",
    "    Loads data from the Excel file, converts it to Documents, and creates a FAISS retriever.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- BUILDING RAG 2: GENERAL Q&A RETRIEVER ---\")\n",
    "    \n",
    "    # 1. Load and process data (using the existing logic)\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, header=1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Using dummy data.\")\n",
    "        df = pd.DataFrame({\n",
    "            'Question': [\"How do I know if my tenancy agreement is valid?\"],\n",
    "            'Answer / Explanation': [\"Must be in writing, signed by both parties, and include essential terms.\"],\n",
    "            'Legal Commentary': [\"Only agreements containing essential terms are enforceable.\"],\n",
    "            'Government Regulation / Explanation': [\"N/A\"]\n",
    "        })\n",
    "    \n",
    "    # 2. Convert Data Rows into LangChain Documents\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        content = textwrap.dedent(f\"\"\"\n",
    "            Question: {row.get('Question', 'N/A')}\n",
    "            Answer/Explanation: {row.get('Answer / Explanation', 'N/A')}\n",
    "            Legal Context: {row.get('Legal Commentary', 'N/A')}\n",
    "            Regulation/Source: {row.get('Government Regulation / Explanation', 'N/A')}\n",
    "        \"\"\").strip()\n",
    "        \n",
    "        # Create a LangChain Document\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": file_path, \"row_index\": index}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    print(f\"Created {len(documents)} General Q&A Documents.\")\n",
    "\n",
    "    print(\"\\n--- GENERAL Q&A CHUNK PREVIEW (First 3) ---\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # 3. Create Vector Store and Retriever\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    print(\"FAISS index for General Q&A created successfully.\")\n",
    "    \n",
    "    # Save the index to the repository\n",
    "    vectorstore.save_local(faiss_index_path)\n",
    "    print(f\"FAISS index saved to {faiss_index_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(FAISS_INDEX_PATH, K):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads the saved FAISS index and returns the retriever object for runtime use.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- LOADING RAG 1 RETRIEVER ---\")\n",
    "    \n",
    "    if not os.path.exists(FAISS_INDEX_PATH):\n",
    "        # This is a critical error if the index should be pre-built\n",
    "        raise FileNotFoundError(\n",
    "            f\"FAISS index not found at {FAISS_INDEX_PATH}. \"\n",
    "            \"Please run 'create_and_save_ideal_index()' first.\"\n",
    "        )\n",
    "\n",
    "    # 1. Load Vector Store\n",
    "    # IMPORTANT: You still need the 'embeddings' model object to load the index\n",
    "    vectorstore = FAISS.load_local(\n",
    "        FAISS_INDEX_PATH, \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization = True # Required by LangChain for loading\n",
    "    )\n",
    "    print(\"FAISS index loaded successfully.\")\n",
    "\n",
    "    # 2. Return Retriever\n",
    "    return vectorstore.as_retriever(search_kwargs = {\"k\": K})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c513d-5db1-4527-bd50-ff99bb92fda6",
   "metadata": {},
   "source": [
    "# Testing RAG 1 and 2\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_NAME = 'Database Requirements.xlsx'\n",
    "DATA_DIRECTORY = \"./TA_template\"\n",
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "GENERAL_QA_FAISS = \"./faiss_index_general_qa\" \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. BUILD THE TWO RAG SYSTEMS\n",
    "    ideal_clauses_knowledgebase = build_ideal_clauses_retriever(DATA_DIRECTORY)\n",
    "    general_qa_knowledgebase = build_general_qa_retriever(FILE_NAME)\n",
    "    ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "    general_qa_retriever = load_retriever(GENERAL_QA_FAISS, K)\n",
    "    print(\"\\n‚úÖ BOTH RAG SYSTEMS ARE READY.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    # --- SIMULATE THE PIPELINE EXECUTION ---\n",
    "    \n",
    "    # --- PHASE 1: CLAUSE IDENTIFICATION & COMPARISON (RAG 1) ---\n",
    "    print(\"\\n--- PHASE 1: COMPARING USER AGREEMENT CLAUSES ---\")\n",
    "    \n",
    "    # 1.1 Simulate a single clause from the user's uploaded document\n",
    "    # In the full app, you would run the text_splitter on the user upload here\n",
    "    user_clause_to_check = \"(b) SECURITY DEPOSIT: The Tenant shall pay the Landlord a security deposit equal to $3,000. This deposit will not accrue interest.\"\n",
    "    \n",
    "    # 1.2 Use RAG 1 to retrieve the Ideal Clause context\n",
    "    comparison_context = ideal_clauses_retriever.invoke(user_clause_to_check)\n",
    "    \n",
    "    # 1.3 LLM Call for Feedback (Simulated)\n",
    "    print(f\"\\nUser Clause: {user_clause_to_check}\")\n",
    "    print(\"\\n[SIMULATED LLM COMPARISON & FEEDBACK GENERATION]\")\n",
    "    print(f\"Retrieved Ideal Context for LLM:\")\n",
    "    for doc in comparison_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now compare the user clause to this context and generate a report.*\n",
    "\n",
    "    # --- PHASE 2: POST-FEEDBACK Q&A (RAG 2) ---\n",
    "    print(\"\\n\\n--- PHASE 2: GENERAL Q&A ---\")\n",
    "    \n",
    "    # 2.1 User enters a follow-up question\n",
    "    user_qa_query = \"What rules apply to my landlord entering my rental?\"\n",
    "    \n",
    "    # 2.2 Use RAG 2 to retrieve the General Q&A context\n",
    "    qa_context = general_qa_retriever.invoke(user_qa_query)\n",
    "    \n",
    "    # 2.3 LLM Call for Answer (Simulated)\n",
    "    print(f\"\\nUser Q&A Query: {user_qa_query}\")\n",
    "    print(\"\\n[SIMULATED LLM ANSWER GENERATION]\")\n",
    "    print(f\"Retrieved General Q&A Context for LLM:\")\n",
    "    for doc in qa_context:\n",
    "        print(f\"  - Length: {len(doc.page_content)}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "    # *A real LLM would now synthesize these into a single, cohesive answer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48039408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking down user uploaded TA\n",
    "def load_and_extract_pdf_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a file is a PDF, loads it, and extracts all text content.\n",
    "    \n",
    "    Args:\n",
    "        file_path: The local path to the user's uploaded file.\n",
    "\n",
    "    Returns:\n",
    "        A single string containing all text from the PDF.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file is not found or is not a PDF.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading and Extracting Text from: {file_path} ---\")\n",
    "\n",
    "    # 1. Basic File Check\n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError(f\"Error: File not found at path: {file_path}\")\n",
    "\n",
    "    # 2. PDF Extension Check (Simple approach)\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(f\"Error: File is not a PDF ('.pdf' extension required).\")\n",
    "\n",
    "    try:\n",
    "        # 3. Use LangChain's PyPDFLoader for robust text extraction\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        \n",
    "        # Load all pages as a list of Document objects\n",
    "        pages = loader.load()\n",
    "        print(f\"Successfully loaded {len(pages)} pages.\")\n",
    "\n",
    "        # 4. Concatenate all page content into a single string\n",
    "        full_text = \"\\n\\n\".join(page.page_content for page in pages)\n",
    "        \n",
    "        # Simple cleanup (optional, but helps with messy PDF parsing)\n",
    "        full_text = re.sub(r'\\s{2,}', ' ', full_text) # Replace multiple spaces/newlines with single space\n",
    "        full_text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', full_text) # Preserve paragraph breaks\n",
    "\n",
    "        print(\"Text extraction complete.\")\n",
    "        return full_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch errors during PDF parsing\n",
    "        raise RuntimeError(f\"An error occurred during PDF text extraction: {e}\")\n",
    "\n",
    "def split_user_document(user_uploaded_text: str, source_name: str = \"User TA\") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Splits the raw text of the user-uploaded tenancy agreement into clause-level chunks.\n",
    "\n",
    "    Args:\n",
    "        user_uploaded_text: The raw string content of the user's document.\n",
    "        source_name: A metadata tag to identify the source (e.g., the filename).\n",
    "\n",
    "    Returns:\n",
    "        A list of LangChain Document objects, one for each clause/chunk.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- SPLITTING USER DOCUMENT: {source_name} ---\")\n",
    "\n",
    "    # The same structure-aware separators used for your Ideal Clauses (RAG 1)\n",
    "    custom_separators = [\n",
    "        \"\\n\\n\",\n",
    "        r\"\\n\\s*[A-Z]+\\s+\\d*\\s*\\.\",\n",
    "        r\"\\n\\s*\\d+\\.\\d*\\s*\",\n",
    "        r\"\\n\\s*\\([a-zA-Z0-9]+\\)\\s*\",\n",
    "        \"\\n\", \" \", \"\"\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        separators=custom_separators,\n",
    "        is_separator_regex=True\n",
    "    )\n",
    "\n",
    "    # 1. Convert the single raw string into a list of Documents (one initial document)\n",
    "    initial_document = [\n",
    "        Document(page_content=user_uploaded_text, metadata={\"source\": source_name})\n",
    "    ]\n",
    "\n",
    "    # 2. Split the document based on the clause structure\n",
    "    user_chunks = text_splitter.split_documents(initial_document)\n",
    "    \n",
    "    print(f\"User TA split into {len(user_chunks)} clause-level chunks.\")\n",
    "    \n",
    "    # Optional: Print a preview of the first few chunks\n",
    "    for i, chunk in enumerate(user_chunks[:3]):\n",
    "        print(f\"  Chunk {i+1} (Length: {len(chunk.page_content)}): {chunk.page_content[:150].replace('\\n', ' ')}...\")\n",
    "    \n",
    "    return user_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84269562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully from environment variable.\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "# --- LLM CONFIGURATION for OpenAI ---\n",
    "# Recommended model for this task (fast and good at structured output)\n",
    "LLM_MODEL = \"gpt-4o-mini\" \n",
    "MAX_RETRIES = 3\n",
    "load_dotenv()  # Load environment variables from a .env file if present\n",
    "# Get the API key from the environment variable\n",
    "api_key_from_env = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key_from_env:\n",
    "    try:\n",
    "        # Pass the key explicitly to ensure the client is initialized correctly\n",
    "        client = OpenAI(api_key=api_key_from_env)\n",
    "        print(\"OpenAI client initialized successfully from environment variable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Failed to initialize OpenAI client even with key. Error: {e}\")\n",
    "        client = None\n",
    "else:\n",
    "    # If the key is not found, print a clear warning and set client to None\n",
    "    print(\"OPENAI_API_KEY environment variable is NOT set.\")\n",
    "    client = None\n",
    "\n",
    "def llm_compare_and_critique_openai(\n",
    "    user_clause_text: str, \n",
    "    ideal_context_docs: List[Document]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Compares a user's tenancy clause against retrieved ideal context using the OpenAI API.\n",
    "    \n",
    "    The function uses structured JSON output enforcement for reliability.\n",
    "\n",
    "    Args:\n",
    "        user_clause_text: The content of the user's clause chunk.\n",
    "        ideal_context_docs: A list of relevant ideal clauses retrieved from the RAG 1 Vector Store.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the feedback, risk level, and suggestions.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return {\n",
    "            \"clause_summary\": \"API Initialization Failed\",\n",
    "            \"risk_level\": \"HIGH\",\n",
    "            \"feedback\": \"OpenAI client is not configured correctly. Check API key.\",\n",
    "            \"suggestion\": \"Set the OPENAI_API_KEY environment variable.\"\n",
    "        }\n",
    "\n",
    "    print(f\"\\n[CRITIQUE] Analyzing clause with GPT: {user_clause_text[:50]}...\")\n",
    "\n",
    "    # 1. FORMAT THE CONTEXT FOR THE LLM\n",
    "    context_str = \"\\n---\\n\".join([doc.page_content for doc in ideal_context_docs])\n",
    "    \n",
    "    # 2. DEFINE SYSTEM INSTRUCTION\n",
    "    # This sets the persona and output constraints.\n",
    "    system_instruction = (\n",
    "        \"You are a world-class legal analyst specializing in residential tenancy agreements. \"\n",
    "        \"Your task is to compare a provided 'User Clause' against 'Ideal Clause Examples' \"\n",
    "        \"and generate structured, actionable feedback. Be concise, professional, and focus only on deviations or missing protections for the tenant. \"\n",
    "        \"Your entire output MUST be a single, valid JSON object that strictly adheres to the provided JSON Schema.\"\n",
    "    )\n",
    "\n",
    "    # 3. DEFINE THE USER QUERY (THE CORE PROMPT)\n",
    "    user_query = textwrap.dedent(f\"\"\"\n",
    "        Please analyze the following 'User Clause' and compare it to the 'Ideal Clause Examples'.\n",
    "\n",
    "        **USER CLAUSE TO CRITIQUE:**\n",
    "        ---\n",
    "        {user_clause_text}\n",
    "        ---\n",
    "\n",
    "        **IDEAL CLAUSE EXAMPLES (RAG Context):**\n",
    "        ---\n",
    "        {context_str}\n",
    "        ---\n",
    "\n",
    "        Based on your comparison, provide the analysis in the specified JSON format.\n",
    "    \"\"\")\n",
    "\n",
    "    # 4. DEFINE THE JSON SCHEMA\n",
    "    response_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"clause_summary\": {\"type\": \"string\", \"description\": \"A brief (1-sentence) summary of the user clause's main topic.\"},\n",
    "            \"risk_level\": {\"type\": \"string\", \"enum\": [\"LOW\", \"MEDIUM\", \"HIGH\"], \"description\": \"The risk level (LOW, MEDIUM, or HIGH) for the tenant based on deviations from the ideal.\"},\n",
    "            \"feedback\": {\"type\": \"string\", \"description\": \"Specific, actionable criticism on what is missing or concerning in the User Clause.\"},\n",
    "            \"suggestion\": {\"type\": \"string\", \"description\": \"A brief sentence on how the user should attempt to modify the clause.\"}\n",
    "        },\n",
    "        \"required\": [\"clause_summary\", \"risk_level\", \"feedback\", \"suggestion\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    # 5. EXECUTE API CALL WITH EXPONENTIAL BACKOFF\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": user_query}\n",
    "                ],\n",
    "                # Use the response_format tool for guaranteed JSON output (GPT-4o/GPT-4 models)\n",
    "                response_format={\"type\": \"json_object\"}, \n",
    "                # Note: The JSON structure is also implicitly constrained by the prompt and system instruction.\n",
    "                temperature=0.0 # Use low temperature for analytical tasks\n",
    "            )\n",
    "            \n",
    "            # Extract and parse the JSON response text\n",
    "            # The entire output text should be a single JSON string\n",
    "            json_text = response.choices[0].message.content\n",
    "            parsed_json = json.loads(json_text)\n",
    "            print(f\"... Successful critique generated on attempt {attempt + 1}.\")\n",
    "            return parsed_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"OpenAI API Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"OpenAI API failed after {MAX_RETRIES} attempts.\")\n",
    "                return {\n",
    "                    \"clause_summary\": \"Analysis Failed\",\n",
    "                    \"risk_level\": \"HIGH\",\n",
    "                    \"feedback\": f\"Unable to generate critique after {MAX_RETRIES} attempts. Error: {e}\",\n",
    "                    \"suggestion\": \"Check your API key, model permissions, and rate limits.\"\n",
    "                }\n",
    "    \n",
    "    # Should be unreachable\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e0c79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_llm_report(data: List[Dict[str, Any]]):\n",
    "    report_lines = [\"# üìÑ Comprehensive Tenancy Agreement Analysis Report\\n\"]\n",
    "    issue_counter = 1\n",
    "    \n",
    "    for clause_feedback in data:\n",
    "        # Check if the structure we expect (top-level 'feedback' key) is present\n",
    "        feedback_data = clause_feedback.get('feedback', {})\n",
    "        \n",
    "        # We need a way to summarize *which clause* this is, but the raw output doesn't \n",
    "        # explicitly provide the clause text. We'll use a placeholder structure for now.\n",
    "        report_lines.append(f\"## üõ†Ô∏è Analysis of Clause {issue_counter}\\n\")\n",
    "        \n",
    "        has_content = False\n",
    "        \n",
    "        # 1. Process Deviations\n",
    "        deviations = feedback_data.get('deviations', [])\n",
    "        if deviations:\n",
    "            report_lines.append(\"### üî¥ Deviations/High Risk Issues:\\n\")\n",
    "            for dev in deviations:\n",
    "                issue = dev.get('issue', 'N/A')\n",
    "                description = dev.get('description', 'No description provided.')\n",
    "                report_lines.append(f\"* **{issue}:** {description}\\n\")\n",
    "            has_content = True\n",
    "\n",
    "        # 2. Process Missing Protections (or similar structures like vague_terms)\n",
    "        missing_protections = feedback_data.get('missing_protections', [])\n",
    "        if not missing_protections:\n",
    "            # Check for other potential keys used by the LLM, like 'missingProtections' (camelCase)\n",
    "            missing_protections = feedback_data.get('missingProtections', [])\n",
    "        \n",
    "        vague_terms = feedback_data.get('vague_terms', [])\n",
    "        \n",
    "        if missing_protections or vague_terms:\n",
    "            report_lines.append(\"\\n### üü° Missing Protections/Vague Terms:\\n\")\n",
    "            \n",
    "            # Combine all non-deviation issues for a clear list\n",
    "            all_other_issues = missing_protections + vague_terms\n",
    "            \n",
    "            for issue_data in all_other_issues:\n",
    "                # The keys change slightly here ('protection' or 'issue')\n",
    "                issue = issue_data.get('protection') or issue_data.get('issue', 'N/A')\n",
    "                description = issue_data.get('description', 'No description provided.')\n",
    "                report_lines.append(f\"* **{issue}:** {description}\\n\")\n",
    "            has_content = True\n",
    "        \n",
    "        # 3. Add separator and increment counter\n",
    "        if has_content:\n",
    "            report_lines.append(\"\\n---\\n\")\n",
    "            issue_counter += 1\n",
    "        \n",
    "    # If the LLM output was perfectly fine, the list might be empty.\n",
    "    if issue_counter == 1:\n",
    "        return \"# ‚úÖ Analysis Complete: No significant deviations found in the provided clauses.\"\n",
    "        \n",
    "    return \"\".join(report_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee16b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "K = 3\n",
    "ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "\n",
    "# Function Integrating LLM and RAG 1\n",
    "def generate_ta_report(USER_UPLOADED_FILE_PATH, ideal_clauses_retriever):\n",
    "    try:\n",
    "        # 1. LOAD & EXTRACT TEXT from the PDF\n",
    "        full_user_document_text = load_and_extract_pdf_text(USER_UPLOADED_FILE_PATH)\n",
    "\n",
    "        # 2. SPLIT the extracted text into clauses\n",
    "        user_clause_chunks = split_user_document(\n",
    "            full_user_document_text, \n",
    "            source_name=USER_UPLOADED_FILE_PATH\n",
    "        )\n",
    "\n",
    "        # 3. Loop through ALL user clauses for comparison (The RAG Core)\n",
    "        feedback_report = []\n",
    "\n",
    "        for user_clause in user_clause_chunks:\n",
    "            # Use RAG 1 to retrieve the Ideal Clause context\n",
    "            comparison_context = ideal_clauses_retriever.invoke(user_clause.page_content)\n",
    "            \n",
    "            # 4. LLM Call for Feedback (Simulated)\n",
    "            feedback = llm_compare_and_critique_openai(user_clause.page_content, comparison_context)\n",
    "            feedback_report.append(feedback)\n",
    "\n",
    "        print(\"\\n--- Phase 1 Complete: Analysis ready. ---\")\n",
    "        # final_report = format_llm_report(feedback_report)\n",
    "        return feedback_report # Changed this from final_report\n",
    "        \n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        # Handle the specific errors raised by the extraction function\n",
    "        print(f\"\\nFATAL ERROR DURING FILE PROCESSING: {e}\")\n",
    "        # You would typically stop processing here and inform the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3402b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "GENERAL_QA_FAISS = \"./faiss_index_general_qa\" \n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "general_qa_retriever = load_retriever(GENERAL_QA_FAISS, K)\n",
    "\n",
    "def answer_contextual_question_openai(\n",
    "    user_question: str, \n",
    "    general_qa_retriever: object,\n",
    "    ta_report: Optional[List[Dict]] = None, # Optional TA Report (Phase 1 output)\n",
    "    past_messages: Optional[List[Dict[str, str]]] = None # Optional Chat History\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Answers a user question using RAG 2, the TA critique report, and chat history.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"OpenAI client is not configured correctly. Check API key.\"\n",
    "\n",
    "    print(f\"\\n[Q&A] Answering contextual question with GPT: {user_question[:50]}...\")\n",
    "\n",
    "    # 1. Use RAG 2 to retrieve the General Q&A context\n",
    "    qa_context = general_qa_retriever.invoke(user_question)\n",
    "    \n",
    "    # 2. Format Contexts\n",
    "    general_context_str = \"\\n---\\n\".join([doc.page_content for doc in qa_context])\n",
    "    report_context_str = (ta_report or []) # Format report if provided\n",
    "\n",
    "    # 3. DEFINE THE LLM MESSAGE LIST (Chat History Integration)\n",
    "    \n",
    "    # Update System Instruction to acknowledge all contexts\n",
    "    system_instruction = (\n",
    "        \"You are an expert legal assistant specializing in residential tenancy agreements. \"\n",
    "        \"Your response must be based on the provided context, which includes the **General Law Context** (RAG data) \"\n",
    "        \"and, if present, the **User Document Analysis Report** (specific critiques). \"\n",
    "        \"Maintain the flow of the conversation history. If the user's question relates to a flagged clause, \"\n",
    "        \"prioritize the information from the Analysis Report. Be concise and professional.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction}\n",
    "    ]\n",
    "\n",
    "    # Add past conversation messages if they exist\n",
    "    if past_messages:\n",
    "        messages.extend(past_messages) # Extends the list with previous turns\n",
    "\n",
    "    # 4. Construct the Final User Query (containing all RAG context)\n",
    "    final_user_prompt = textwrap.dedent(f\"\"\"\n",
    "        Please answer the FINAL USER QUESTION using the context provided below.\n",
    "\n",
    "        **THIS IS A REPORT ON THE USER'S TENANCY AGREEMENT (Specific Critiques):**\n",
    "        ---\n",
    "        {report_context_str}\n",
    "        \n",
    "        **GENERAL Q&A CONTEXT (RAG Retrieved):**\n",
    "        ---\n",
    "        {general_context_str}\n",
    "        ---\n",
    "\n",
    "        **FINAL USER QUESTION:** {user_question}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Add the final, context-rich user prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": final_user_prompt})\n",
    "\n",
    "    # 5. EXECUTE API CALL WITH EXPONENTIAL BACKOFF\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "         \n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_MODEL,\n",
    "                messages=messages, # Now uses the full history list\n",
    "                temperature=0.0\n",
    "            )\n",
    "            answer_text = response.choices[0].message.content\n",
    "            \n",
    "            print(f\"... Successful answer generated on attempt {attempt + 1}.\")\n",
    "            return answer_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"OpenAI API Error: {e}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"OpenAI API failed after {MAX_RETRIES} attempts.\")\n",
    "                return f\"Unable to generate answer after {MAX_RETRIES} attempts. Error: {e}\"\n",
    "    \n",
    "    return \"Unknown error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d087056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING RAG 1 RETRIEVER ---\n",
      "FAISS index loaded successfully.\n",
      "\n",
      "‚úÖ RAG 1 IS READY.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Loading and Extracting Text from: ./TA_sample/TA3.pdf ---\n",
      "Successfully loaded 2 pages.\n",
      "Text extraction complete.\n",
      "\n",
      "--- SPLITTING USER DOCUMENT: ./TA_sample/TA3.pdf ---\n",
      "User TA split into 8 clause-level chunks.\n",
      "  Chunk 1 (Length: 633): Disclaimer: This is a general document which may not be appropriate for use in all cases. When in doubt, please seek legal advice. In the event of a d...\n",
      "  Chunk 2 (Length: 514): Singapore This Agreement is made on the day of between (hereinafter known as ‚ÄúThe Landlord‚Äù) on the one part and (hereinafter known as ‚ÄúThe Tenant‚Äù) o...\n",
      "  Chunk 3 (Length: 715): 2. Such deposit shall be refundable at the end of the term after deductions for damages caused by the Tenant, if any. 3. The Tenant must pay the month...\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: Disclaimer: This is a general document which may n...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: Singapore This Agreement is made on the day of bet...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 2. Such deposit shall be refundable at the end of ...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 6. The above premises are strictly not for immoral...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 7.2 In respect of any change in the particulars, i...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 9. The Tenant is not allowed to sublet the premise...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 12. The Landlord shall pay their representing sale...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "[CRITIQUE] Analyzing clause with GPT: 16. The Tenant must produce original/photocopy of ...\n",
      "... Successful critique generated on attempt 1.\n",
      "\n",
      "--- Phase 1 Complete: Analysis ready. ---\n"
     ]
    }
   ],
   "source": [
    "# Testing RAG 1 with LLM for full TA Report Generation\n",
    "\n",
    "# --- Configuration ---\n",
    "IDEAL_CLAUSES_FAISS = \"./faiss_index_ideal_clauses\"\n",
    "K = 3 # Number of top results to retrieve\n",
    "\n",
    "# Initialize shared embedding model once\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "# Initialize RAG 1\n",
    "ideal_clauses_retriever = load_retriever(IDEAL_CLAUSES_FAISS, K)\n",
    "\n",
    "print(\"\\n‚úÖ RAG 1 IS READY.\")\n",
    "print(\"--------------------------------------------------\")\n",
    "    \n",
    "# --- Placeholder: Replace this with the actual path to the user's uploaded file ---\n",
    "USER_UPLOADED_FILE_PATH = \"./TA_sample/TA3.pdf\" \n",
    "    # Note: In a real-world application, this path comes from your web/API framework after the user submits the file.\n",
    "\n",
    "# Generate the TA Report\n",
    "ta_report = generate_ta_report(USER_UPLOADED_FILE_PATH, ideal_clauses_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b414a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chng Zhi Quan\\OneDrive\\Desktop\\Masters\\Courses\\Sem 1\\DS5105-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üë®‚Äçüîß Calling Parser Node ---\n",
      "\n",
      "--- ‚úÖ Graph Finished. Final State: ---\n",
      "\n",
      "--- Clause 1 ---\n",
      "Title: 1. PROPERTY\n",
      "Text: The landlord agrees to let the property at 123 Main St, Anytown....\n",
      "\n",
      "--- Clause 2 ---\n",
      "Title: 2. TERM\n",
      "Text: The tenancy shall be for a period of 12 months, commencing on 1st Dec ...\n",
      "\n",
      "--- Clause 3 ---\n",
      "Title: 3. RENT\n",
      "Text: The rent shall be $2,000 per month, payable on the 1st of each month....\n",
      "\n",
      "--- Clause 4 ---\n",
      "Title: 4. SECURITY DEPOSIT\n",
      "Text: The tenant shall pay a deposit of $2,000. The landlord may deduct from...\n",
      "\n",
      "--- Clause 5 ---\n",
      "Title: 5. MAINTENANCE AND REPAIRS\n",
      "Text: a. The tenant shall keep the interior of the property in good and clea...\n",
      "\n",
      "Successfully parsed 5 clauses.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# --- 1. Set API Key ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- 2. Define Pydantic Models (Our \"Template\") ---\n",
    "\n",
    "class Clause(BaseModel):\n",
    "    \"\"\"A single clause from the tenancy agreement.\"\"\"\n",
    "    clause_title: str = Field(description=\"The clause number and title, e.g., '5. Maintenance' or '3. Security Deposit'\")\n",
    "    clause_text: str = Field(description=\"The full, verbatim text of the clause.\")\n",
    "\n",
    "class ParsedAgreement(BaseModel):\n",
    "    \"\"\"The full tenancy agreement, parsed into a list of clauses.\"\"\"\n",
    "    clauses: List[Clause]\n",
    "\n",
    "# --- 3. Define the LangGraph State ---\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    This is the \"conveyor belt\" for our data.\n",
    "    \n",
    "    Attributes:\n",
    "        raw_text: The full, raw text of the tenancy agreement (Input).\n",
    "        clauses: The list of parsed Clause objects (Output of this node).\n",
    "    \"\"\"\n",
    "    raw_text: str\n",
    "    clauses: List[Clause]\n",
    "\n",
    "\n",
    "# --- 4. The Parser Node Function ---\n",
    "\n",
    "def parse_agreement_node(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    The first node in our graph.\n",
    "    It takes the raw_text from the state and uses an LLM to parse it\n",
    "    into a structured list of Clause objects.\n",
    "    \"\"\"\n",
    "    print(\"--- üë®‚Äçüîß Calling Parser Node ---\")\n",
    "    \n",
    "    # Get the raw text from the state\n",
    "    raw_text = state[\"raw_text\"]\n",
    "    \n",
    "    # Set up the LLM. \n",
    "    # gpt-4o-mini is fast, cheap, and excellent at structured output.\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    # Use .with_structured_output() to force the LLM to return our Pydantic object\n",
    "    structured_llm = llm.with_structured_output(ParsedAgreement)\n",
    "    \n",
    "    # Create the prompt\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert legal document parser. Your sole job is to read the\n",
    "    following tenancy agreement and convert it into a structured list of its\n",
    "    distinct clauses.\n",
    "    \n",
    "    Carefully extract each clause, using its number and title (e.g., '4. Security Deposit')\n",
    "    as the 'clause_title' and the full text of that clause as the 'clause_text'.\n",
    "    Ensure you capture all clauses from start to finish.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Please parse this tenancy agreement:\\n\\n{agreement_text}\")\n",
    "    ])\n",
    "    \n",
    "    # Create the chain\n",
    "    parser_chain = prompt | structured_llm\n",
    "    \n",
    "    # Invoke the chain\n",
    "    parsed_agreement = parser_chain.invoke({\"agreement_text\": raw_text})\n",
    "    \n",
    "    # Return a dictionary to update the state\n",
    "    return {\"clauses\": parsed_agreement.clauses}\n",
    "\n",
    "\n",
    "# --- 5. Example of how to build and run the graph ---\n",
    "\n",
    "# A simple TA to test with\n",
    "SAMPLE_TA = \"\"\"\n",
    "TENANCY AGREEMENT\n",
    "This agreement is made between Mr. John Smith (Landlord) and Ms. Jane Doe (Tenant).\n",
    "\n",
    "1. PROPERTY\n",
    "The landlord agrees to let the property at 123 Main St, Anytown.\n",
    "\n",
    "2. TERM\n",
    "The tenancy shall be for a period of 12 months, commencing on 1st Dec 2025.\n",
    "\n",
    "3. RENT\n",
    "The rent shall be $2,000 per month, payable on the 1st of each month.\n",
    "\n",
    "4. SECURITY DEPOSIT\n",
    "The tenant shall pay a deposit of $2,000. The landlord may deduct from this deposit any costs for repairs or unpaid rent. The deposit will be returned within 14 days of the tenancy ending, less any deductions.\n",
    "\n",
    "5. MAINTENANCE AND REPAIRS\n",
    "a. The tenant shall keep the interior of the property in good and clean condition.\n",
    "b. The tenant shall be responsible for servicing the air-conditioning units once every 3 months.\n",
    "c. The landlord shall be responsible for all external and structural repairs.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # We will only build a 1-node graph for now\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add our parser node\n",
    "    workflow.add_node(\"parser\", parse_agreement_node)\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"parser\")\n",
    "    \n",
    "    # The parser is the only node, so it ends\n",
    "    workflow.add_edge(\"parser\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Run the graph!\n",
    "    inputs = {\"raw_text\": SAMPLE_TA}\n",
    "    \n",
    "    # .invoke() runs the graph. \n",
    "    # LangFuse would automatically trace this.\n",
    "    final_state = app.invoke(inputs)\n",
    "    \n",
    "    # --- 6. Print the results ---\n",
    "    print(\"\\n--- ‚úÖ Graph Finished. Final State: ---\")\n",
    "    \n",
    "    # The output is a clean list of Pydantic objects\n",
    "    parsed_clauses = final_state['clauses']\n",
    "    \n",
    "    for i, clause in enumerate(parsed_clauses):\n",
    "        print(f\"\\n--- Clause {i+1} ---\")\n",
    "        print(f\"Title: {clause.clause_title}\")\n",
    "        print(f\"Text: {clause.clause_text[:70]}...\") # Print snippet\n",
    "        \n",
    "    print(f\"\\nSuccessfully parsed {len(parsed_clauses)} clauses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
